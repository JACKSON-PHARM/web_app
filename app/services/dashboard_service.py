"""
Dashboard Service
Provides data for dashboard views: new arrivals, priority items, etc.
"""
import logging
import sqlite3
from datetime import datetime, timedelta
import pandas as pd
import os
import glob

logger = logging.getLogger(__name__)

class DashboardService:
    """Service for dashboard data queries"""
    
    def __init__(self, db_manager):
        self.db_manager = db_manager
        # ALL data is in Supabase PostgreSQL - no SQLite support
        # Verify we're using PostgreSQL
        if not (hasattr(db_manager, 'connection_string') or hasattr(db_manager, 'pool') or 'PostgresDatabaseManager' in str(type(db_manager))):
            raise ValueError("DashboardService requires PostgresDatabaseManager. All data is stored in Supabase PostgreSQL.")
        self.is_postgres = True  # Always PostgreSQL now
        # Inventory analysis CSV for ABC mapping (same as stock view)
        self.inventory_analysis_path = os.path.join(
            os.path.dirname(os.path.dirname(__file__)),
            "resources", "templates", "Inventory_Analysis.csv"
        )
        self._abc_cache = None
        self._inventory_analysis_cache = None
        # Removed _cached_db_path - not needed for PostgreSQL
    
    def _normalize_query(self, query: str) -> str:
        """Convert SQLite-style ? placeholders to PostgreSQL-style %s"""
        # Always PostgreSQL - replace ? with %s
        return query.replace('?', '%s')
    
    def _execute_query(self, query: str, params: tuple = None) -> list:
        """Execute query using PostgreSQL database manager"""
        normalized_query = self._normalize_query(query)
        return self.db_manager.execute_query(normalized_query, params)

    # Removed _get_database_path - not needed for PostgreSQL

    def _load_inventory_analysis(self) -> pd.DataFrame:
        """Load Inventory_Analysis from Supabase (or CSV fallback)"""
        if self._inventory_analysis_cache is not None:
            return self._inventory_analysis_cache
        
        try:
            # For PostgreSQL, load from database table
            if self.is_postgres:
                # Try inventory_analysis_new first (then inventory_analysis as fallback)
                table_name = None
                try:
                    # Check which table exists
                    check_result = self._execute_query("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name IN ('inventory_analysis_new', 'inventory_analysis')
                        ORDER BY CASE WHEN table_name = 'inventory_analysis_new' THEN 1 ELSE 2 END
                        LIMIT 1
                    """)
                    if check_result:
                        table_name = check_result[0]['table_name']
                        logger.info(f"Using table: {table_name}")
                except Exception as e:
                    logger.warning(f"Could not check for inventory_analysis tables: {e}")
                
                if table_name:
                    try:
                        # Get column names to build appropriate query
                        # Use parameterized query for table name check
                        columns_result = self._execute_query("""
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_schema = 'public' 
                            AND table_name = %s
                        """, (table_name,))
                        available_columns = [row['column_name'] for row in columns_result] if columns_result else []
                        
                        # Build SELECT list based on available columns
                        select_cols = []
                        if 'company_name' in available_columns:
                            select_cols.append('company_name')
                        elif 'company' in available_columns:
                            select_cols.append('company as company_name')
                        
                        if 'branch_name' in available_columns:
                            select_cols.append('branch_name')
                        elif 'branch' in available_columns:
                            select_cols.append('branch as branch_name')
                        
                        # Add other columns that might exist
                        for col in ['item_code', 'item_name', 'total_pieces_sold', 'total_sales_value', 
                                   'sale_days_nosun', 'base_amc', 'adjusted_amc', 'days_since_first_sale',
                                   'days_since_last_sale', 'stock_days_nosun', 'snapshot_days_nosun',
                                   'stock_availability_pct', 'abc_class', 'abc_priority',
                                   'customer_appeal', 'modal_units_sold', 'last_stock_level',
                                   'ideal_stock_pieces', 'stock_recommendation']:
                            if col in available_columns:
                                select_cols.append(col)
                        
                        if select_cols:
                            # Table name is validated, safe to use in f-string
                            # Escape table name to prevent SQL injection (though we've validated it)
                            safe_table_name = table_name.replace('"', '""')  # Escape quotes
                            query = f"""
                                SELECT {', '.join(select_cols)}
                                FROM "{safe_table_name}"
                                LIMIT 1000000
                            """
                            results = self._execute_query(query)
                            
                            if results:
                                df = pd.DataFrame(results)
                                self._inventory_analysis_cache = df
                                logger.info(f"âœ… Loaded {len(df)} items from Supabase {table_name} table")
                                return df
                            else:
                                logger.warning(f"âš ï¸ {table_name} table is empty in Supabase. Run load_inventory_analysis_to_supabase.py to load data.")
                        else:
                            logger.warning(f"âš ï¸ {table_name} table exists but has no recognized columns")
                    except Exception as e:
                        logger.warning(f"Could not load from Supabase {table_name} table: {e}")
                        logger.info("Falling back to CSV file...")
                else:
                    logger.warning("âš ï¸ Neither inventory_analysis_new nor inventory_analysis table found in Supabase")
                    logger.info("Falling back to CSV file...")
            
            # Fallback to CSV file (for SQLite or if Supabase table doesn't exist)
            if os.path.exists(self.inventory_analysis_path):
                df = pd.read_csv(self.inventory_analysis_path)
                # Cache it
                self._inventory_analysis_cache = df
                logger.info(f"âœ… Loaded {len(df)} items from Inventory_Analysis.csv")
                return df
            else:
                logger.warning(f"âš ï¸ Inventory_Analysis.csv not found at {self.inventory_analysis_path}")
                return pd.DataFrame()
        except Exception as e:
            logger.error(f"âŒ Error loading Inventory_Analysis: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return pd.DataFrame()

    def _load_abc_map(self) -> pd.DataFrame:
        """Load ABC classification map from Inventory_Analysis.csv or latest stock_view_*.xlsx"""
        if self._abc_cache is not None:
            return self._abc_cache
        try:
            # 1) Try CSV first (Inventory_Analysis.csv)
            if os.path.exists(self.inventory_analysis_path):
                try:
                    df = pd.read_csv(self.inventory_analysis_path)
                    # Normalize column names we need: ItemCode, ABC
                    cols = {c.lower(): c for c in df.columns}
                    code_col = cols.get('item_code') or cols.get('itemcode') or cols.get('item')
                    abc_col = cols.get('abc_class') or cols.get('abc') or cols.get('class')
                    if code_col and abc_col:
                        df = df[[code_col, abc_col]].rename(
                            columns={code_col: 'item_code', abc_col: 'abc_class'}
                        )
                        # Clean up: remove rows with missing values
                        df = df.dropna(subset=['item_code', 'abc_class'])
                        df['item_code'] = df['item_code'].astype(str).str.strip()
                        df['abc_class'] = df['abc_class'].astype(str).str.strip().str.upper()
                        # Remove duplicates by item_code, keeping first occurrence
                        df = df.drop_duplicates(subset=['item_code'], keep='first')
                        self._abc_cache = df
                        logger.info(f"Loaded {len(df)} ABC mappings from CSV (deduplicated)")
                        return df
                except Exception as ex:
                    logger.warning(f"Failed to load ABC from CSV: {ex}")
            
            # 2) Fallback: latest stock_view_*.xlsx export (column T holds ABC class)
            app_root = os.path.dirname(os.path.dirname(__file__))
            # Check both app root and resources/templates folder
            xlsx_candidates = []
            # First check resources/templates folder (preferred location)
            resources_xlsx = glob.glob(os.path.join(app_root, "resources", "templates", "stock_view_*.xlsx"))
            xlsx_candidates.extend(resources_xlsx)
            # Then check app root
            root_xlsx = glob.glob(os.path.join(app_root, "stock_view_*.xlsx"))
            xlsx_candidates.extend(root_xlsx)
            # Sort by modification time (most recent first)
            xlsx_candidates = sorted(
                xlsx_candidates,
                key=os.path.getmtime,
                reverse=True
            )
            for path in xlsx_candidates:
                try:
                    logger.info(f"Trying to load ABC from Excel: {path}")
                    df_x = pd.read_excel(path)
                    logger.info(f"Excel file has {len(df_x.columns)} columns: {list(df_x.columns[:25])}")
                    
                    # Normalize column names
                    cols = {c.lower(): c for c in df_x.columns}
                    code_col = cols.get('item_code') or cols.get('itemcode') or cols.get('item')
                    
                    # Try to find ABC column by name first
                    abc_col = cols.get('abc_class') or cols.get('abc') or cols.get('class')
                    
                    # If not found by name, try column T (index 19, zero-based)
                    if not abc_col and len(df_x.columns) > 19:
                        abc_col = df_x.columns[19]
                        logger.info(f"Using column index 19 (T) as ABC: {abc_col}")
                    
                    if code_col and abc_col:
                        # Extract only needed columns
                        df_x = df_x[[code_col, abc_col]].copy()
                        df_x = df_x.rename(columns={code_col: 'item_code', abc_col: 'abc_class'})
                        
                        # Clean up: remove rows with missing values
                        df_x = df_x.dropna(subset=['item_code', 'abc_class'])
                        df_x['item_code'] = df_x['item_code'].astype(str).str.strip()
                        df_x['abc_class'] = df_x['abc_class'].astype(str).str.strip().str.upper()
                        
                        # Remove duplicates, keeping first
                        df_x = df_x.drop_duplicates(subset=['item_code'], keep='first')
                        
                        self._abc_cache = df_x
                        logger.info(f"Loaded {len(df_x)} ABC mappings from Excel file {os.path.basename(path)}")
                        return df_x
                    else:
                        logger.warning(f"Could not find item_code or ABC column in {path}. Code: {code_col}, ABC: {abc_col}")
                except Exception as ex:
                    logger.warning(f"Failed to load ABC from {path}: {ex}", exc_info=True)
            
            # Fallback: empty DataFrame
            logger.warning("No ABC mapping found in CSV or Excel files")
            self._abc_cache = pd.DataFrame(columns=['item_code', 'abc_class'])
            return self._abc_cache
        except Exception as e:
            logger.error(f"Error loading ABC map: {e}", exc_info=True)
            self._abc_cache = pd.DataFrame(columns=['item_code', 'abc_class'])
            return self._abc_cache
    
    def get_new_arrivals_this_week(self, source_branch: str, source_company: str,
                                   target_branch: str, target_company: str, limit: int = 50):
        """
        Get new arrivals (supplier invoices) from BABA DOGO HQ this week (last 7 days)
        Shows items received at HQ regardless of selected branch, with branch stock info
        
        Args:
            source_branch: Always "BABA DOGO HQ" (hardcoded)
            source_company: Company name (NILA or DAIMA)
            target_branch: Selected branch name (to show stock levels)
            target_company: Target company name
            limit: Maximum number of items to return
            
        Returns:
            DataFrame with columns: item_code, item_name, quantity, document_date, document_number, 
                                   source_type, branch_stock, hq_stock, abc_class, etc.
        """
        try:
            # Always use BABA DOGO HQ as source branch
            hq_branch = "BABA DOGO HQ"
            
            # Use database manager instead of direct SQLite connection
            # First, find what companies actually have invoices for BABA DOGO HQ
            # (invoices might be for a different company than the selected one)
            available_companies_result = self._execute_query("""
                SELECT DISTINCT company 
                FROM supplier_invoices 
                WHERE branch = ?
            """, (hq_branch,))
            available_companies = [row['company'] for row in available_companies_result] if available_companies_result else []
            
            if not available_companies:
                # Check if table exists and has any data
                total_invoices_result = self._execute_query("SELECT COUNT(*) as count FROM supplier_invoices")
                total_invoices = total_invoices_result[0]['count'] if total_invoices_result else 0
                logger.warning(f"No supplier invoices found for {hq_branch} (total invoices in table: {total_invoices})")
                logger.warning(f"Database: Supabase PostgreSQL")
                return pd.DataFrame(columns=['item_code', 'item_name', 'quantity', 'document_date', 
                                            'document_number', 'source_type', 'branch_stock_pieces', 
                                            'branch_stock_packs', 'hq_stock_pieces', 'hq_stock_packs'])
            
            logger.info(f"Found supplier invoices for companies: {available_companies} at {hq_branch}")
            
            # Calculate date range: last 7 days from TODAY
            # This shows actual "this week" arrivals relative to current date
            today = datetime.now().date()
            end_date = today
            start_date = end_date - timedelta(days=7)
            
            # Check if there are any invoices in the last 7 days
            recent_count_result = self._execute_query("""
                SELECT COUNT(*) as count
                FROM supplier_invoices 
                WHERE branch = %s AND document_date >= %s AND document_date <= %s
            """, (hq_branch, start_date, end_date))
            recent_count = recent_count_result[0]['count'] if recent_count_result else 0
            
            # Always use today's date range - don't fall back to old dates
            # This ensures we show current data when available, and empty when it's not
            # Check what the most recent invoice date actually is (for logging/debugging)
            max_date_result = self._execute_query("""
                SELECT MAX(document_date) as max_date
                FROM supplier_invoices 
                WHERE branch = %s
            """, (hq_branch,))
            max_date = max_date_result[0]['max_date'] if max_date_result and max_date_result[0].get('max_date') else None
            
            if max_date:
                if isinstance(max_date, str):
                    max_date_obj = datetime.strptime(max_date, '%Y-%m-%d').date()
                elif isinstance(max_date, datetime):
                    max_date_obj = max_date.date()
                else:
                    max_date_obj = max_date
                
                days_old = (today - max_date_obj).days
                if days_old > 7:
                    logger.warning(f"âš ï¸ Most recent invoice date ({max_date_obj}) is {days_old} days old. No invoices in last 7 days. Refresh data to see new arrivals.")
                else:
                    logger.info(f"âœ… Found {recent_count} invoices in last 7 days (most recent: {max_date_obj})")
            else:
                logger.warning(f"âš ï¸ No invoices found in database for {hq_branch}. Refresh data to see new arrivals.")
            
            # If no invoices in last 7 days, return empty DataFrame (don't show old data)
            if recent_count == 0:
                logger.info(f"No invoices in last 7 days from today ({start_date} to {end_date}). Returning empty results. Refresh data to see new arrivals.")
                return pd.DataFrame(columns=['item_code', 'item_name', 'quantity', 'document_date', 
                                            'document_number', 'source_type', 'branch_stock_pieces', 
                                            'branch_stock_packs', 'hq_stock_pieces', 'hq_stock_packs'])
            else:
                logger.info(f"Found {recent_count} invoices in last 7 days. Querying new arrivals from {hq_branch} for this week: {start_date} to {end_date}")
            
            # Query supplier invoices from BABA DOGO HQ (last 7 days from most recent date)
            # Show all items received at HQ, regardless of target branch stock
            # Include branch stock from selected branch for reference
            # Use current_stock table only (stock_data table doesn't exist in Supabase)
            hq_stock_result = self._execute_query("SELECT COUNT(*) as count FROM current_stock WHERE branch = %s", (hq_branch,))
            hq_stock_count = hq_stock_result[0]['count'] if hq_stock_result else 0
            
            # Query using current_stock (will show zeros if table is empty)
            invoice_query = """
                SELECT 
                    si.item_code,
                    MAX(si.item_name) AS item_name,
                    MAX(si.units) as quantity,
                    MAX(COALESCE(cs_hq.pack_size, 1)) AS pack_size,
                    MAX(ROUND(si.units / NULLIF(COALESCE(cs_hq.pack_size, 1), 0))) AS quantity_packs,
                    MAX(si.document_date) AS document_date,
                    MAX(si.document_number) AS document_number,
                    'Supplier Invoice' as source_type,
                    COALESCE(MAX(cs_branch.stock_pieces), 0) AS branch_stock_pieces,
                    COALESCE(MAX(cs_branch.stock_pieces / NULLIF(COALESCE(cs_branch.pack_size, 1), 0)), 0) AS branch_stock_packs,
                    MAX(COALESCE(cs_hq.stock_pieces, 0)) AS hq_stock_pieces,
                    COALESCE(MAX(cs_hq.stock_pieces / NULLIF(COALESCE(cs_hq.pack_size, 1), 0)), 0) AS hq_stock_packs,
                    MAX(cs_hq.pack_size) AS pack_size_hq,
                    MAX(cs_branch.pack_size) AS pack_size_branch
                FROM supplier_invoices si
                LEFT JOIN current_stock cs_hq
                    ON cs_hq.item_code = si.item_code 
                   AND cs_hq.company = si.company 
                   AND cs_hq.branch = %s
                LEFT JOIN current_stock cs_branch
                    ON cs_branch.item_code = si.item_code
                   AND cs_branch.company = %s
                   AND cs_branch.branch = %s
                WHERE si.branch = %s
                    AND si.document_date >= %s AND si.document_date <= %s
                GROUP BY si.item_code
                ORDER BY MAX(si.document_date) DESC, MAX(si.document_number) DESC
                LIMIT %s
            """
            
            if hq_stock_count == 0:
                logger.warning(f"current_stock table is empty for {hq_branch}. Stock values will show as 0. Please refresh data.")
            
            # Execute query using current_stock only
            results = self._execute_query(
                invoice_query,
                (hq_branch, target_company, target_branch, hq_branch, start_date, end_date, limit * 2)
            )
            
            combined = pd.DataFrame(results) if results else pd.DataFrame()
            logger.info(f"New arrivals query returned {len(combined)} items from {hq_branch} (this week: {start_date} to {end_date})")
            
            # If no results in last 7 days, try last 30 days as fallback
            if combined.empty:
                logger.info(f"No invoices in last 7 days, trying last 30 days from today")
                start_date_fallback = today - timedelta(days=30)
                results = self._execute_query(
                    invoice_query,
                    (hq_branch, target_company, target_branch, hq_branch, start_date_fallback, end_date, limit * 2)
                )
                combined = pd.DataFrame(results) if results else pd.DataFrame()
                logger.info(f"Fallback query returned {len(combined)} items from {hq_branch} (last 30 days)")
            
            if combined.empty:
                logger.warning(f"No new arrivals found at {hq_branch} (checked last 30 days from today: {start_date} to {end_date})")
                return pd.DataFrame(columns=['item_code', 'item_name', 'quantity', 'document_date', 
                                            'document_number', 'source_type', 'branch_stock_pieces', 
                                            'branch_stock_packs', 'hq_stock_pieces', 'hq_stock_packs'])
            
            # Attach ABC class, AMC, and other data from Inventory_Analysis.csv
            inventory_df = self._load_inventory_analysis()
            if not inventory_df.empty:
                # Filter inventory data for the target branch/company if available
                branch_inventory = inventory_df[
                    (inventory_df.get('branch_name', '') == target_branch) & 
                    (inventory_df.get('company_name', '') == target_company)
                ].copy()
                
                if branch_inventory.empty:
                    # Try without branch filter
                    branch_inventory = inventory_df[
                        inventory_df.get('company_name', '') == target_company
                    ].copy()
                
                if not branch_inventory.empty:
                    # Merge available columns
                    merge_cols = ['item_code']
                    if 'abc_class' in branch_inventory.columns:
                        merge_cols.append('abc_class')
                    if 'adjusted_amc' in branch_inventory.columns:
                        merge_cols.append('adjusted_amc')
                    elif 'base_amc' in branch_inventory.columns:
                        merge_cols.append('base_amc')
                    if 'customer_appeal' in branch_inventory.columns:
                        merge_cols.append('customer_appeal')
                    if 'stock_recommendation' in branch_inventory.columns:
                        merge_cols.append('stock_recommendation')
                    
                    available_cols = [col for col in merge_cols if col in branch_inventory.columns]
                    if available_cols:
                        combined = combined.merge(
                            branch_inventory[available_cols].drop_duplicates('item_code'),
                            on='item_code',
                            how='left'
                        )
                        
                        # Rename columns for consistency
                        if 'adjusted_amc' in combined.columns:
                            combined['amc'] = combined['adjusted_amc']
                        elif 'base_amc' in combined.columns:
                            combined['amc'] = combined['base_amc']
                        
                        if 'stock_recommendation' in combined.columns:
                            combined['stock_comment'] = combined['stock_recommendation']
            
            # Ensure required columns exist
            if 'abc_class' not in combined.columns:
                combined['abc_class'] = ''
            if 'amc' not in combined.columns:
                combined['amc'] = 0
            if 'customer_appeal' not in combined.columns:
                combined['customer_appeal'] = 1.0
            if 'stock_comment' not in combined.columns:
                combined['stock_comment'] = ''
            
            # Fill NaN values
            combined['abc_class'] = combined['abc_class'].fillna('')
            combined['amc'] = pd.to_numeric(combined['amc'], errors='coerce').fillna(0)
            combined['customer_appeal'] = pd.to_numeric(combined['customer_appeal'], errors='coerce').fillna(1.0)
            combined['stock_comment'] = combined['stock_comment'].fillna('')
            combined['branch_stock_pieces'] = pd.to_numeric(combined['branch_stock_pieces'], errors='coerce').fillna(0)
            combined['branch_stock_packs'] = pd.to_numeric(combined['branch_stock_packs'], errors='coerce').fillna(0)
            combined['hq_stock_pieces'] = pd.to_numeric(combined['hq_stock_pieces'], errors='coerce').fillna(0)
            combined['hq_stock_packs'] = pd.to_numeric(combined['hq_stock_packs'], errors='coerce').fillna(0)
            
            # Sort by date descending and limit
            combined = combined.sort_values('document_date', ascending=False).head(limit)
            
            logger.info(f"Found {len(combined)} new arrivals from {hq_branch} (last 7 days from most recent date: {max_date})")
            return combined
            
        except Exception as e:
            logger.error(f"Error getting new arrivals: {e}", exc_info=True)
            return pd.DataFrame(columns=['item_code', 'item_name', 'quantity', 'document_date', 
                                        'document_number', 'source_type'])
    
    def get_priority_items_between_branches(self, target_branch: str, target_company: str,
                                            source_branch: str, source_company: str,
                                            limit: int = 50):
        """
        Get priority items: Items that are IN STOCK at source branch (HQ) but NOT in selected branch
        and are fast moving (Class A, B, or C)
        
        Priority items are those where:
        - Item is available (stock_pieces > 0) in source branch (HQ)
        - Item is NOT available (stock_pieces <= 0) in target branch (selected branch)
        - Item is fast moving (ABC Class A, B, or C)
        
        Args:
            target_branch: Target branch name (selected branch - where we DON'T have stock)
            target_company: Target company name
            source_branch: Source branch name (e.g., "BABA DOGO HQ" - where we HAVE stock)
            source_company: Source company name
            limit: Maximum number of items to return
            
        Returns:
            DataFrame with columns: item_code, item_name, source_stock_packs, branch_name,
                                  target_stock_packs, abc_class, stock_level_pct, amc_pieces, pack_size
        """
        try:
            # Use database manager instead of direct SQLite connection
            # Use current_stock table only (stock_data table doesn't exist in Supabase)
            source_stock_result = self._execute_query(
                "SELECT COUNT(*) as count FROM current_stock WHERE branch = %s AND company = %s",
                (source_branch, source_company)
            )
            source_stock_count = source_stock_result[0]['count'] if source_stock_result else 0
            
            # Use current_stock if available
            if source_stock_count > 0:
                # Check if materialized view exists and use it for faster queries
                try:
                    conn_check = self.db_manager.get_connection()
                    cursor_check = conn_check.cursor()
                    cursor_check.execute("""
                        SELECT EXISTS (
                            SELECT FROM pg_matviews 
                            WHERE schemaname = 'public' 
                            AND matviewname = 'priority_items_materialized'
                        )
                    """)
                    has_materialized_view = cursor_check.fetchone()[0]
                    cursor_check.close()
                    self.db_manager.put_connection(conn_check)
                    
                    if has_materialized_view:
                        logger.info("âœ… Using priority_items_materialized for faster query")
                        # Use materialized view - much faster! All columns pre-computed
                        query = """
                            SELECT 
                                item_code,
                                item_name,
                                source_branch,
                                source_company,
                                source_stock_pieces,
                                target_branch,
                                target_company,
                                target_stock_pieces,
                                pack_size,
                                abc_class,
                                amc_pieces,
                                stock_comment,
                                last_order_date,
                                stock_level_pct
                            FROM priority_items_materialized
                            WHERE UPPER(TRIM(source_branch)) = UPPER(TRIM(%s))
                                AND UPPER(TRIM(source_company)) = UPPER(TRIM(%s))
                                AND UPPER(TRIM(target_branch)) = UPPER(TRIM(%s))
                                AND UPPER(TRIM(target_company)) = UPPER(TRIM(%s))
                            ORDER BY source_stock_pieces DESC
                            LIMIT %s
                        """
                        query = self._normalize_query(query)
                        params = (
                            source_branch, source_company,
                            target_branch, target_company,
                            limit
                        )
                    else:
                        raise Exception("No materialized view")
                except:
                    # Fallback to regular query
                    logger.info(f"Using current_stock table for priority items (found {source_stock_count} records for {source_branch})")
                    # Simplified query - remove complex subquery for last_order_date to avoid timeouts
                    query = """
                    SELECT 
                        cs_source.item_code,
                        MAX(cs_source.item_name) AS item_name,
                        MAX(cs_source.stock_pieces) AS source_stock_pieces,
                        MAX(cs_source.pack_size) AS source_pack_size,
                        COALESCE(MAX(cs_target.stock_pieces), 0) AS target_stock_pieces,
                        COALESCE(MAX(cs_target.pack_size), MAX(cs_source.pack_size)) AS pack_size,
                        0 AS stock_level_pct,
                        NULL::date AS last_order_date
                    FROM current_stock cs_source
                    LEFT JOIN current_stock cs_target
                        ON cs_target.item_code = cs_source.item_code
                        AND UPPER(TRIM(cs_target.company)) = UPPER(TRIM(%s))
                        AND UPPER(TRIM(cs_target.branch)) = UPPER(TRIM(%s))
                    WHERE UPPER(TRIM(cs_source.branch)) = UPPER(TRIM(%s))
                        AND UPPER(TRIM(cs_source.company)) = UPPER(TRIM(%s))
                        AND cs_source.stock_pieces > 0
                        AND (
                            cs_target.stock_pieces IS NULL 
                            OR cs_target.stock_pieces <= 0
                            OR cs_target.stock_pieces < 1000
                        )
                    GROUP BY cs_source.item_code
                    ORDER BY MAX(cs_source.stock_pieces) DESC
                    LIMIT %s
                """
                # Normalize query for database type
                query = self._normalize_query(query)
                params = (
                    target_company, target_branch,  # For cs_target join
                    source_branch, source_company,  # For cs_source WHERE
                    limit  # For LIMIT
                )
            else:
                logger.warning(f"No stock data found for {source_branch} ({source_company}) in current_stock table.")
                logger.info(f"Please sync stock data using 'Refresh All Data' -> 'Stock' to populate priority items.")
                return pd.DataFrame(columns=['item_code', 'item_name', 'source_stock_packs', 'branch_name',
                                            'target_stock_packs', 'abc_class', 'stock_level_pct', 'amc_packs', 
                                            'pack_size', 'last_order_date'])
            # Check if source and target are the same branch (would return no results)
            if source_branch == target_branch and source_company == target_company:
                logger.warning(f"âš ï¸ Source and target branches are the same ({source_branch}), cannot find priority items. Priority items require different branches.")
                logger.info(f"ðŸ’¡ Tip: Select a different target branch (e.g., a retail branch) to see items that need to be transferred from {source_branch}.")
                return pd.DataFrame(columns=['item_code', 'item_name', 'source_stock_packs', 'branch_name',
                                            'target_stock_packs', 'abc_class', 'stock_level_pct', 'amc_packs', 
                                            'pack_size', 'last_order_date'])
            
            # Execute query using database manager
            logger.info(f"ðŸ” Executing priority items query: source={source_branch} ({source_company}), target={target_branch} ({target_company})")
            logger.info(f"ðŸ“Š Query will be normalized for PostgreSQL")
            
            # Check if we're using materialized view
            using_materialized_view = 'priority_items_materialized' in query.upper()
            
            # Query was already normalized above when params were set
            normalized_query = self._normalize_query(query)
            logger.debug(f"Normalized query (first 200 chars): {normalized_query[:200]}")
            try:
                results = self._execute_query(normalized_query, params)
            except Exception as query_error:
                logger.error(f"Priority items query failed: {query_error}")
                import traceback
                logger.error(traceback.format_exc())
                # Return empty DataFrame on error to avoid breaking the UI
                return pd.DataFrame(columns=['item_code', 'item_name', 'source_stock_packs', 'branch_name',
                                            'target_stock_packs', 'abc_class', 'stock_level_pct', 'amc_packs', 
                                            'pack_size', 'last_order_date'])
            
            # Convert results to DataFrame
            df = pd.DataFrame(results) if results else pd.DataFrame()
            
            logger.info(f"ðŸ“Š Query returned {len(df)} rows before filtering")
            
            # If using materialized view, skip complex processing - data is already complete
            if using_materialized_view and not df.empty:
                logger.info("âœ… Using priority_items_materialized - data already includes ABC, AMC, and stock levels")
                # Materialized view already has all columns, just ensure proper types and column names
                # Map column names to expected format
                column_mapping = {
                    'source_stock_pieces': 'source_stock_pieces',
                    'target_stock_pieces': 'target_stock_pieces',
                    'amc': 'amc_pieces',  # Materialized view has amc_pieces as amc
                    'amc_pieces': 'amc_pieces'
                }
                
                # Ensure numeric columns are numeric
                numeric_cols = ['source_stock_pieces', 'target_stock_pieces', 'pack_size', 'amc_pieces', 'stock_level_pct']
                for col in numeric_cols:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
                
                # Calculate packs from pieces
                df['source_stock_packs'] = df.apply(
                    lambda row: row['source_stock_pieces'] / row['pack_size'] if row['pack_size'] > 0 else 0,
                    axis=1
                )
                df['target_stock_packs'] = df.apply(
                    lambda row: row['target_stock_pieces'] / row['pack_size'] if row['pack_size'] > 0 else 0,
                    axis=1
                )
                df['amc_packs'] = df.apply(
                    lambda row: row['amc_pieces'] / row['pack_size'] if row['pack_size'] > 0 and row['amc_pieces'] > 0 else 0,
                    axis=1
                )
                
                # Map branch_name from target_branch
                if 'target_branch' in df.columns and 'branch_name' not in df.columns:
                    df['branch_name'] = df['target_branch']
                
                # Ensure required columns exist
                for col in ['abc_class', 'stock_comment', 'last_order_date']:
                    if col not in df.columns:
                        df[col] = '' if col in ['abc_class', 'stock_comment'] else None
                
                logger.info(f"âœ… Processed {len(df)} items from priority_items_materialized")
                # Skip the complex filtering below - materialized view already has correct data
                return df
            
            if df.empty:
                logger.info(f"No priority items found: source={source_branch} has stock, target={target_branch} doesn't")
                logger.info(f"ðŸ’¡ This could mean:")
                logger.info(f"   1. Target branch ({target_branch}) already has all items in stock")
                logger.info(f"   2. Source branch ({source_branch}) doesn't have stock for items that target needs")
                logger.info(f"   3. Try selecting different branches")
                return pd.DataFrame(columns=['item_code', 'item_name', 'source_stock_packs', 'branch_name',
                                            'target_stock_packs', 'abc_class', 'stock_level_pct', 'amc_packs', 
                                            'pack_size', 'last_order_date'])
            
            # Aggressive duplicate removal by item_code (should not happen with GROUP BY, but safety check)
            initial_count = len(df)
            # Check for duplicates before removal
            duplicates = df[df.duplicated(subset=['item_code'], keep=False)]
            if not duplicates.empty:
                logger.warning(f"Found {len(duplicates)} duplicate item_codes in priority query before removal")
            
            df = df.drop_duplicates(subset=['item_code'], keep='first')
            if len(df) < initial_count:
                logger.warning(f"Removed {initial_count - len(df)} duplicate items from priority query (after GROUP BY)")
            
            # Verify we have unique item_codes
            if len(df) != len(df['item_code'].unique()):
                logger.error(f"Still have duplicates after drop_duplicates! {len(df)} rows vs {len(df['item_code'].unique())} unique item_codes")
                # Force unique by taking first occurrence
                df = df.groupby('item_code').first().reset_index()
            
            # Verify source stock > 0 (should already be filtered in SQL, but double-check)
            df = df[df['source_stock_pieces'] > 0]
            
            # Load ABC class and AMC from Inventory_Analysis for TARGET branch (items that need restocking)
            inventory_df = self._load_inventory_analysis()
            if not inventory_df.empty:
                # Filter by TARGET branch and company (items that need restocking at target)
                branch_inventory = inventory_df[
                    (inventory_df['branch_name'] == target_branch) & 
                    (inventory_df['company_name'] == target_company)
                ].copy()
                
                if branch_inventory.empty:
                    # Try without branch filter (use company only)
                    logger.info(f"No inventory analysis found for target branch {target_branch}, trying company-wide")
                    branch_inventory = inventory_df[
                        inventory_df['company_name'] == target_company
                    ].copy()
                
                if not branch_inventory.empty:
                    # Prepare columns to merge - get ABC class from TARGET branch
                    merge_cols = ['item_code']
                    
                    # Add ABC class (from target branch - items that need restocking)
                    if 'abc_class' in branch_inventory.columns:
                        merge_cols.append('abc_class')
                    
                    # Add AMC (use adjusted_amc if available, else base_amc) - from target branch
                    if 'adjusted_amc' in branch_inventory.columns:
                        merge_cols.append('adjusted_amc')
                    elif 'base_amc' in branch_inventory.columns:
                        merge_cols.append('base_amc')
                    
                    # Merge available columns
                    available_cols = [col for col in merge_cols if col in branch_inventory.columns]
                    if available_cols:
                        branch_inventory_dedup = branch_inventory[available_cols].drop_duplicates('item_code')
                        df = df.merge(branch_inventory_dedup, on='item_code', how='left')
                        
                        # Set ABC class (from target branch)
                        if 'abc_class' in df.columns:
                            df['abc_class'] = df['abc_class'].fillna('')
                        else:
                            df['abc_class'] = ''
                        
                        # Set AMC (use adjusted_amc if available, else base_amc) - from target branch
                        if 'adjusted_amc' in df.columns:
                            df['amc_pieces'] = df['adjusted_amc'].fillna(0)
                        elif 'base_amc' in df.columns:
                            df['amc_pieces'] = df['base_amc'].fillna(0)
                        else:
                            df['amc_pieces'] = 0
                    else:
                        df['abc_class'] = ''
                        df['amc_pieces'] = 0
                else:
                    logger.warning(f"No inventory analysis found for target branch {target_branch} ({target_company})")
                    df['abc_class'] = ''
                    df['amc_pieces'] = 0
            else:
                # Fallback: Load ABC class from ABC map
                logger.warning("No inventory analysis table found, using ABC map fallback")
                abc_map = self._load_abc_map()
                if not abc_map.empty:
                    # Ensure ABC map is deduplicated by item_code before merging
                    abc_map = abc_map.drop_duplicates(subset=['item_code'], keep='first')
                    # Merge ABC class by item_code
                    df = df.merge(abc_map, on='item_code', how='left')
                    # Fill missing ABC class with empty string
                    df['abc_class'] = df['abc_class'].fillna('')
                else:
                    df['abc_class'] = ''
                df['amc_pieces'] = 0
            
            # Filter to A/B/C only (fast moving items) - ABC class from TARGET branch
            before_abc = len(df)
            df = df[df['abc_class'].isin(['A', 'B', 'C'])]
            logger.info(f"After ABC filter (A/B/C from target branch): {len(df)} items (was {before_abc})")
            
            # Log detailed diagnostics before reorder filter
            logger.info(f"ðŸ“Š Before reorder level filter: {len(df)} items")
            if len(df) > 0:
                logger.info(f"   Sample items: {df[['item_code', 'item_name', 'target_stock_pieces', 'source_stock_pieces']].head(3).to_dict('records')}")
            
            if df.empty:
                logger.info(f"No priority items found (no A/B/C items in {source_branch} but not in {target_branch})")
                return pd.DataFrame(columns=['item_code', 'item_name', 'source_stock_packs', 'branch_name',
                                            'target_stock_packs', 'abc_class', 'stock_level_pct', 'amc_packs', 
                                            'pack_size', 'last_order_date'])
            
            # Calculate reorder level based on ABC class and filter items below reorder level
            # Reorder thresholds: A=50%, B=30%, C=25% of AMC
            df['reorder_threshold'] = df.apply(
                lambda row: row['amc_pieces'] * {'A': 0.5, 'B': 0.3, 'C': 0.25}.get(row.get('abc_class', 'C'), 0.25),
                axis=1
            )
            
            # Filter to items that are out of stock OR below reorder level
            before_reorder_filter = len(df)
            df = df[
                (df['target_stock_pieces'] <= 0) | 
                (df['target_stock_pieces'] < df['reorder_threshold'])
            ]
            logger.info(f"After reorder level filter: {len(df)} items (was {before_reorder_filter})")

            # Ensure pack_size is available - get from source or target stock, or use default 1
            if 'pack_size' not in df.columns or df['pack_size'].isna().all():
                # Try to get pack_size from source_pack_size
                if 'source_pack_size' in df.columns:
                    df['pack_size'] = df['source_pack_size'].fillna(1)
                else:
                    df['pack_size'] = 1
            
            # Replace 0 or NaN pack_size with 1 to avoid division by zero
            df['pack_size'] = df['pack_size'].replace(0, 1).fillna(1)
            source_pack_size = df['source_pack_size'].replace(0, 1).fillna(1) if 'source_pack_size' in df.columns else df['pack_size']
            
            # Calculate packs from pieces using pack_size (round to whole packs)
            df['source_stock_packs'] = (df['source_stock_pieces'] / source_pack_size).round(0).astype(int)
            df['target_stock_packs'] = (df['target_stock_pieces'] / df['pack_size']).round(0).astype(int)
            
            # Convert AMC from pieces to packs (using target branch pack_size) - round to 2 decimals for display
            df['amc_packs'] = (df['amc_pieces'] / df['pack_size']).round(2)
            
            # Recalculate stock_level_pct using AMC from Inventory_Analysis
            df['stock_level_pct'] = df.apply(
                lambda row: (row['target_stock_pieces'] / row['amc_pieces'] * 100) if row['amc_pieces'] > 0 else 0,
                axis=1
            )
            
            # Add branch_name column (target branch)
            df['branch_name'] = target_branch
            
            # Format dates - convert to datetime and format as string for display
            if 'last_order_date' in df.columns:
                df['last_order_date'] = pd.to_datetime(df['last_order_date'], errors='coerce')
                # Format as YYYY-MM-DD or empty string if NaT
                df['last_order_date'] = df['last_order_date'].dt.strftime('%Y-%m-%d').replace('NaT', '').replace('nan', '')
            
            # Drop temporary columns
            df = df.drop(columns=['source_pack_size', 'reorder_threshold'], errors='ignore')
            
            # Sort by ABC class (A first, then B, then C), then by target stock ascending (lowest first)
            df['abc_sort'] = df['abc_class'].map({'A': 1, 'B': 2, 'C': 3}).fillna(99)
            df = df.sort_values(['abc_sort', 'target_stock_packs', 'source_stock_packs'], ascending=[True, True, False])
            df = df.drop(columns=['abc_sort'])
            
            logger.info(f"Found {len(df)} priority items: {source_branch} has stock, {target_branch} needs restocking")
            return df[['item_code', 'item_name', 'source_stock_packs', 'branch_name',
                      'target_stock_packs', 'abc_class', 'stock_level_pct', 'amc_packs', 
                      'pack_size', 'last_order_date']].head(limit)
                
        except Exception as e:
            logger.error(f"Error getting priority items: {e}", exc_info=True)
            return pd.DataFrame(columns=['item_code', 'item_name', 'source_stock_packs', 'branch_name',
                                        'target_stock_packs', 'abc_class', 'stock_level_pct', 'amc_packs', 
                                        'pack_size', 'last_order_date'])

